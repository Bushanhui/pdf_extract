import re
import pandas as pd
import unicodedata
import datetime
from pathlib import Path
from tqdm import tqdm
from collections import Counter, defaultdict
from typing import Set, Dict, List, Any
import warnings
import numpy as np
import utils

# Pandas ê²½ê³  ë©”ì‹œì§€ ë¹„í™œì„±í™” (SettingWithCopyWarning ë“±)
warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)

# ==============================================================================
# ì œê³µëœ ëª¨ë“  í—¬í¼ í•¨ìˆ˜ ë° í´ë˜ìŠ¤ë¥¼ ì—¬ê¸°ì— í¬í•¨ì‹œí‚µë‹ˆë‹¤.
# (ì´ íŒŒì¼ í•˜ë‚˜ë§Œìœ¼ë¡œ ëª¨ë“  ê¸°ëŠ¥ì´ ë™ì‘í•˜ë„ë¡ self-containedí•˜ê²Œ ë§Œë“­ë‹ˆë‹¤)
# ==============================================================================

# Global variables for numbering patterns - will be initialized by load_config
COMMON_NUMBERING_PATTERNS = []
TEXT_ONLY_NUMBERING_PATTERNS = []

def load_config(config_path=None):
    """Config íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì „ì—­ íŒ¨í„´ ë³€ìˆ˜ë“¤ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global COMMON_NUMBERING_PATTERNS, TEXT_ONLY_NUMBERING_PATTERNS
    
    config = utils.load_config(config_path)
    
    # ê³µí†µ íŒ¨í„´ ì´ˆê¸°í™”
    COMMON_NUMBERING_PATTERNS = [re.compile(pattern, re.IGNORECASE) for pattern in config['numbering_patterns']['common']]
    
    # í…ìŠ¤íŠ¸ ì „ìš© íŒ¨í„´ ì´ˆê¸°í™”  
    TEXT_ONLY_NUMBERING_PATTERNS = [re.compile(pattern) for pattern in config['numbering_patterns']['text_only']]
    
    return config

# ê¸°ë³¸ config ë¡œë“œ (í•„ìš”ì‹œ ëª…ì‹œì ìœ¼ë¡œ í˜¸ì¶œ)
# load_config()


# [ìˆ˜ì •ë¨] ë¬¸ë§¥(context)ì— ë”°ë¼ ë„˜ë²„ë§ ì œê±°ë¥¼ ë‹¤ë¥´ê²Œ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
def remove_numbering(sentence, context='text', config_path=None):
    """
    ë¬¸ë§¥(context)ì— ë”°ë¼ ë‹¤ë¥¸ ë„˜ë²„ë§ íŒ¨í„´ ì„¸íŠ¸ë¥¼ ì ìš©í•˜ì—¬ ë¬¸ì¥ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
    :param sentence: ì²˜ë¦¬í•  ë¬¸ì¥
    :param context: 'text' ë˜ëŠ” 'table'. ê¸°ë³¸ê°’ì€ 'text'.
    :param config_path: config íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
    """
    global COMMON_NUMBERING_PATTERNS, TEXT_ONLY_NUMBERING_PATTERNS
    
    # config_pathê°€ ì œê³µë˜ë©´ ìƒˆë¡œ ë¡œë“œ
    if config_path is not None:
        load_config(config_path)
    
    if not sentence or not isinstance(sentence, str):
        return sentence

    # ë¬¸ë§¥ì— ë”°ë¼ ì ìš©í•  íŒ¨í„´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê²°ì •
    patterns_to_apply = COMMON_NUMBERING_PATTERNS
    if context == 'text':
        # í…ìŠ¤íŠ¸ ë¬¸ë§¥ì—ì„œëŠ” ëª¨ë“  íŒ¨í„´ì„ ìˆœì„œëŒ€ë¡œ ì ìš© (ìœ„í—˜í•œ íŒ¨í„´ì€ ë‚˜ì¤‘ì—)
        patterns_to_apply = patterns_to_apply + TEXT_ONLY_NUMBERING_PATTERNS

    cleaned_sentence = sentence
    for pattern in patterns_to_apply:
        match = pattern.match(cleaned_sentence)
        if match:
            # íŒ¨í„´ì´ ì¼ì¹˜í•˜ë©´ í•´ë‹¹ ë¶€ë¶„ë§Œ ì œê±°í•˜ê³  ë‹¤ìŒ íŒ¨í„´ìœ¼ë¡œ ë„˜ì–´ê°€ì§€ ì•ŠìŒ
            cleaned_sentence = pattern.sub(' ', cleaned_sentence, count=1)
            break
            
    return cleaned_sentence.lstrip()

def normalize_quotes_simple(text):
    if not text or not isinstance(text, str): return text
    result = []
    for char in text:
        if ord(char) in {0x201C, 0x201D, 0x201E, 0x201F}: result.append('"')
        elif ord(char) in {0x2018, 0x2019, 0x0060}: result.append("'")
        else: result.append(char)
    return ''.join(result)

def normalize_quotes_in_dataframe(df, columns=['kor_sentence_cleaned', 'eng_sentence_cleaned']):
    df_copy = df.copy()
    for col in columns:
        if col in df_copy.columns:
            df_copy[col] = df_copy[col].apply(lambda x: normalize_quotes_simple(x) if pd.notna(x) else x)
    return df_copy

def extract_punctuation(text):
    if not text or not isinstance(text, str): return []
    end_periods = re.findall(r'\.$', text)
    other_punct = re.findall(r'[!?:;%\"`~â€¦]', text)
    return end_periods + other_punct

def compare_punctuation(kor_text, eng_text):
    kor_punct = extract_punctuation(kor_text) if kor_text else []
    eng_punct = extract_punctuation(eng_text) if eng_text else []
    kor_counter, eng_counter = Counter(kor_punct), Counter(eng_punct)
    differences = {}
    for punct in set(kor_punct + eng_punct):
        kor_count, eng_count = kor_counter.get(punct, 0), eng_counter.get(punct, 0)
        if kor_count != eng_count:
            differences[punct] = {'korean': kor_count, 'english': eng_count, 'diff': kor_count - eng_count}
    return {
        'match_type': 'match' if kor_punct == eng_punct else 'no_match',
        'kor_punct_str': ''.join(kor_punct),
        'eng_punct_str': ''.join(eng_punct),
        'differences': differences,
    }

def analyze_punctuation_dataframe(df, kor_col='kor_sentence_cleaned', eng_col='eng_sentence_cleaned'):
    df_result = df.copy()
    results = [compare_punctuation(row.get(kor_col), row.get(eng_col)) for _, row in df.iterrows()]
    df_result['kor_punct'] = [r['kor_punct_str'] for r in results]
    df_result['eng_punct'] = [r['eng_punct_str'] for r in results]
    df_result['punct_match_type'] = [r['match_type'] for r in results]
    df_result['punct_differences'] = [r['differences'] for r in results]
    return df_result

class IntegratedNumberComparer:
    def __init__(self):
        self._NUMERIC_PATTERNS = [
            re.compile(r'\d{1,3}(?:,\d{3})+(?:\.\d+)?'), 
            re.compile(r'\d+\.\d+'), 
            re.compile(r'\d+-\d+'), 
            re.compile(r'\b(\d+)(?:st|nd|rd|th)\b', re.IGNORECASE), 
            re.compile(r'\d+')
        ]
        eng_mappings = {'1':['one','first','primary','January','Jan','single','uni','once','mono'],'2':['two','second','secondary','February','Feb','dual','double','bi','twin','pair','couple','twice','di'],'3':['three','third','March','Mar','triple','tri','trio','thrice'],'4':['four','fourth','April','Apr','quad','tetra','quartet','quarter'],'5':['five','fifth','May','penta','quintet'],'6':['six','sixth','June','Jun','hexa','sextet'],'7':['seven','seventh','July','Jul','septet','hepta'],'8':['eight','eighth','August','Aug','octet','octa'],'9':['nine','ninth','September','Sep','Sept','nona'],'10':['ten','tenth','October','Oct','deca','decade'],'11':['eleven','eleventh','November','Nov'],'12':['twelve','twelfth','December','Dec','dozen'],'13':['thirteen','thirteenth'],'14':['fourteen','fourteenth'],'15':['fifteen','fifteenth'],'16':['sixteen','sixteenth'],'17':['seventeen','seventeenth'],'18':['eighteen','eighteenth'],'19':['nineteen','nineteenth'],'20':['twenty','twentieth'],'30':['thirty','thirtieth'],'40':['forty','fortieth'],'50':['fifty','fiftieth'],'60':['sixty','sixtieth'],'70':['seventy','seventieth'],'80':['eighty','eightieth'],'90':['ninety','ninetieth'],'100':['hundred'],'1000':['thousand'],'0':['zero','oh']}
        self.text_to_num_map = {text: num for num, texts in eng_mappings.items() for text in (texts + [t.capitalize() for t in texts if t and not t[0].isupper()])}
        kor_mappings = {'2': ['ì´ì¤‘', 'ë”ë¸”']}
        self.kor_text_to_num_map = {text: num for num, texts in kor_mappings.items() for text in texts}
        kor_keywords = self.kor_text_to_num_map.keys()
        if kor_keywords:
            self._KOR_NUMERIC_TEXT_PATTERN = re.compile('|'.join(re.escape(k) for k in kor_keywords))
        else:
            self._KOR_NUMERIC_TEXT_PATTERN = None

    # [ìˆ˜ì •ë¨] ë°˜í™˜ íƒ€ì…ì„ List[str]ë¡œ ë³€ê²½í•˜ê³ , set ëŒ€ì‹  listë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    def _extract_korean_numeric_tokens(self, text: str) -> List[str]:
        if not isinstance(text, str) or not text: return []
        nums = [] # âœ… set() -> []
        for pat in self._NUMERIC_PATTERNS: 
            # âœ… nums.add -> nums.append
            text = pat.sub(lambda m: nums.append(m.group(0).replace(',','')) or ' ', text)
        return nums

    # [ìˆ˜ì •ë¨] ë°˜í™˜ íƒ€ì…ì„ List[str]ë¡œ ë³€ê²½í•˜ê³ , set ëŒ€ì‹  listë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    def _extract_english_numeric_tokens(self, text: str) -> List[str]:
        if not isinstance(text, str) or not text: return []
        # âœ… set() -> []
        nums = [digit for sup, digit in {'â°':'0','Â¹':'1','Â²':'2','Â³':'3','â´':'4','âµ':'5','â¶':'6','â·':'7','â¸':'8','â¹':'9'}.items() if sup in text]
        for pat in self._NUMERIC_PATTERNS: 
            # âœ… nums.add -> nums.append
            text = pat.sub(lambda m: nums.append(m.group(0).replace(',','')) or ' ', text)
        return nums

    # [ìˆ˜ì •ë¨] ë°˜í™˜ íƒ€ì…ì„ List[str]ë¡œ ë³€ê²½í•˜ê³ , set ëŒ€ì‹  listë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    def _map_textual_numbers(self, text: str) -> List[str]:
        if not isinstance(text, str) or not text: return []
        found_nums = []
        tokens = re.findall(r'[\w-]+', text.lower())  # ì†Œë¬¸ìë¡œ ë³€í™˜
        
        # ë§¤í•‘ë„ ì†Œë¬¸ìë¡œ ë³€í™˜í•´ì„œ ë¹„êµ
        lower_text_to_num_map = {k.lower(): v for k, v in self.text_to_num_map.items()}
        
        for token in tokens:
            # 1. ë‹¨ë… ë‹¨ì–´ ë§¤ì¹­ (ì†Œë¬¸ì ë§¤í•‘ ì‚¬ìš©)
            if token in lower_text_to_num_map:
                found_nums.append(lower_text_to_num_map[token])
                continue
            
            # 2. í•˜ì´í”ˆì´ ìˆëŠ” í† í° ì²˜ë¦¬
            if '-' in token:
                token_parts = token.split('-')
                for part in token_parts:
                    for num_word, num_val in lower_text_to_num_map.items():
                        if part.startswith(num_word) and len(num_word) >= 2:
                            found_nums.append(num_val)
                            break
                    else:
                        continue
                    break
                continue
            
            # 3. í•˜ì´í”ˆ ì—†ëŠ” ë‹¨ì–´ - ë‹¨ì–´ ì‹œì‘ìœ¼ë¡œë§Œ ë§¤ì¹­
            for num_word, num_val in lower_text_to_num_map.items():
                if token.startswith(num_word) and len(num_word) >= 2:
                    found_nums.append(num_val)
                    break
                        
        return found_nums

    # [ìˆ˜ì •ë¨] ë°˜í™˜ íƒ€ì…ì„ List[str]ë¡œ ë³€ê²½í•˜ê³ , set ëŒ€ì‹  listë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    def _map_korean_textual_numbers(self, text: str) -> List[str]:
        if not isinstance(text, str) or not text or not self._KOR_NUMERIC_TEXT_PATTERN:
            return []
        found_tokens = self._KOR_NUMERIC_TEXT_PATTERN.findall(text)
        # âœ… set comprehension -> list comprehension
        return [self.kor_text_to_num_map[token] for token in found_tokens]            

    # [ìˆ˜ì •ë¨] Counter ê°ì²´ë¥¼ ë°›ì•„ ë¹ˆë„ìˆ˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” ë¡œì§ìœ¼ë¡œ ì „ë©´ ìˆ˜ì •
    def _calculate_number_differences(self, kor_counter: Counter, eng_counter: Counter) -> Dict[str, Dict[str, int]]:
        differences = {}
        all_nums = sorted(list(kor_counter.keys() | eng_counter.keys()))

        for num in all_nums:
            kor_count = kor_counter.get(num, 0)
            eng_count = eng_counter.get(num, 0)
            if kor_count != eng_count:
                differences[num] = {
                    'korean': kor_count, 
                    'english': eng_count, 
                    'diff': kor_count - eng_count
                }
        return differences

    # [ìˆ˜ì •ë¨] ë¦¬ìŠ¤íŠ¸ì™€ Counterë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë¹„êµ ë¡œì§ì„ ì¬êµ¬ì„±
    def compare(self, kor_text: str, eng_text: str) -> Dict[str, Any]:
        # 1. ì‹¤ì œ ìˆ«ìë§Œ ë¨¼ì € ì¶”ì¶œ (í…ìŠ¤íŠ¸ ë§¤í•‘ ì—†ì´)
        kor_nums = self._extract_korean_numeric_tokens(kor_text)
        eng_nums = self._extract_english_numeric_tokens(eng_text)

        # 2. ì´ˆê¸° Counter ìƒì„±
        kor_counter = Counter(kor_nums)
        eng_counter = Counter(eng_nums)

        # 3. ìƒíƒœ ì •ì˜ í•¨ìˆ˜ (Counter ë¹„êµ ê¸°ë°˜)
        def get_status(k_counter, e_counter):
            if not k_counter and not e_counter:
                return "no_numbers"
            if k_counter == e_counter:
                return "all_match"
            # êµì§‘í•©(Counterì˜ & ì—°ì‚°)ì´ ìˆìœ¼ë©´ partial_match
            if bool(k_counter & e_counter):
                return "partial_match"
            return "no_match"

        # 4. ì´ˆê¸° ìƒíƒœ í™•ì¸
        initial_status = get_status(kor_counter, eng_counter)

        # 5. ë§¤í•‘ ì‹œë„ ì¡°ê±´: no_numbersë‚˜ all_matchê°€ ì•„ë‹ ë•Œë§Œ
        if initial_status != "no_numbers" and initial_status != "all_match":
            # ì¡°ê±´ A: í•œìª½ë§Œ ìˆ«ì ìˆìŒ
            if bool(kor_counter) != bool(eng_counter):
                if kor_counter and not eng_counter:
                    # í•œêµ­ì–´ë§Œ ìˆ«ì ìˆìŒ â†’ ì˜ì–´ì—ì„œ í…ìŠ¤íŠ¸â†’ìˆ«ì ë§¤í•‘
                    eng_nums.extend(self._map_textual_numbers(eng_text))
                else:
                    # ì˜ì–´ë§Œ ìˆ«ì ìˆìŒ â†’ í•œêµ­ì–´ì—ì„œ í…ìŠ¤íŠ¸â†’ìˆ«ì ë§¤í•‘
                    kor_nums.extend(self._map_korean_textual_numbers(kor_text))
            # ì¡°ê±´ B: ë‘˜ ë‹¤ ìˆ«ì ìˆì§€ë§Œ ë¶ˆì¼ì¹˜
            elif kor_counter and eng_counter:
                # ì–‘ìª½ ëª¨ë‘ í…ìŠ¤íŠ¸â†’ìˆ«ì ë§¤í•‘ ì‹œë„
                kor_nums.extend(self._map_korean_textual_numbers(kor_text))
                eng_nums.extend(self._map_textual_numbers(eng_text))

            # ì¬ê³„ì‚°
            kor_counter = Counter(kor_nums)
            eng_counter = Counter(eng_nums)

        # 6. ê²°ê³¼ ë°˜í™˜
        return {
            "kor_numbers": sorted(kor_nums),
            "eng_numbers_after_mapping": sorted(eng_nums),
            "number_match_status": get_status(kor_counter, eng_counter),
            "num_differences": self._calculate_number_differences(kor_counter, eng_counter)
        }

def analyze_dataframe_with_comparer(df: pd.DataFrame, comparer: IntegratedNumberComparer, kor_col: str = "kor_sentence_cleaned", eng_col: str = "eng_sentence_cleaned") -> pd.DataFrame:
    results = [comparer.compare(str(row.get(kor_col, "")), str(row.get(eng_col, ""))) for _, row in df.iterrows()]
    out = df.copy()
    out["number_match_status"] = [r["number_match_status"] for r in results]
    out["kor_numbers"] = [", ".join(r["kor_numbers"]) for r in results]
    out["eng_numbers_after_mapping"] = [", ".join(r["eng_numbers_after_mapping"]) for r in results]
    out["num_differences"] = [r["num_differences"] for r in results]
    return out

# [ìˆ˜ì •ë¨] ë‹¨ì–´ ë¹ˆë„ìˆ˜ê¹Œì§€ ë¹„êµí•˜ë„ë¡ ë¡œì§ ì „ë©´ ìˆ˜ì •
def compare_english_words(kor_text: str, eng_text: str) -> Dict[str, Any]:
    word_pattern = r'[a-zA-Z]+(?:[.-][a-zA-Z]+)*'
    normalize = lambda t: re.sub(r'[^a-z0-9]', '', t.lower())
    roman_map = {'i':'1','ii':'2','iii':'3','iv':'4','v':'5','vi':'6','vii':'7','viii':'8','ix':'9','x':'10'}

    # 1. [ìˆ˜ì •] í•œêµ­ì–´ ë¬¸ì¥ì—ì„œ ì˜ì–´ ë‹¨ì–´ë¥¼ ì°¾ì•„ Counterë¡œ ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    kor_eng_words_orig = re.findall(word_pattern, kor_text or "")
    if not kor_eng_words_orig:
        return {"kor_eng_words": [], "status": "no_eng_in_kor", "differences": {}}

    # ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ Counter ìƒì„±
    kor_word_counter = Counter(w.lower() for w in kor_eng_words_orig)

    # 2. [ìˆ˜ì •] ì˜ì–´ ë¬¸ì¥ì—ì„œë„ ëª¨ë“  ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ Counterë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
    #    - ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ì°¾ì€ ë‹¨ì–´ì™€, '1st' -> 'st' ê°™ì€ ì ‘ë¯¸ì‚¬ë„ í¬í•¨í•©ë‹ˆë‹¤.
    eng_lower = (eng_text or "").lower()
    eng_words = re.findall(word_pattern, eng_lower) + re.findall(r'\d+([a-zA-Z]+)', eng_lower)
    eng_word_counter = Counter(eng_words)
    eng_nums = set(re.findall(r'\d+', (eng_text or "")))

    # 3. [ìˆ˜ì •] ì˜ì–´ Counterì— ë‹¨ì–´ì˜ ë³€í˜•(ë³µìˆ˜í˜•, í•˜ì´í”ˆ ë“±)ì„ ì¶”ê°€ë¡œ ê³ ë ¤í•©ë‹ˆë‹¤.
    #    - ì˜ˆë¥¼ ë“¤ì–´ 'axes'ê°€ ìˆìœ¼ë©´ 'axis'ë„ ì¹´ìš´íŠ¸ ë˜ë„ë¡ í•©ë‹ˆë‹¤.
    #    - ì£¼ì˜: ì›ë³¸ Counterë¥¼ ë³µì‚¬í•´ì„œ ì‚¬ìš©í•´ì•¼ ë£¨í”„ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.
    for word, count in list(eng_word_counter.items()):
        # ë³µìˆ˜í˜• ì²˜ë¦¬ ('ies' -> 'y', 's' -> '')
        if word.endswith('ies'):
            eng_word_counter[word[:-3] + 'y'] += count
        elif len(word) > 2 and word.endswith('s'):
            eng_word_counter[word[:-1]] += count
        # í•˜ì´í”ˆ ì²˜ë¦¬ ('state-of-the-art' -> 'state', 'of', 'the', 'art')
        if '-' in word:
            for part in word.split('-'):
                if part:
                    eng_word_counter[part] += count

    # 4. [ìˆ˜ì •] Counter ëº„ì…ˆìœ¼ë¡œ ë¶€ì¡±í•œ ë‹¨ì–´ì™€ ê°œìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    missing_counter = Counter()
    for word, kor_count in kor_word_counter.items():
        # ë¡œë§ˆ ìˆ«ì íŠ¹ë³„ ì²˜ë¦¬ (e.g., 'i', 'ii')
        is_roman_match = word in roman_map and roman_map[word] in eng_nums
        if is_roman_match:
            continue

        # ì˜ì–´ ë¬¸ì¥ì—ì„œ í•´ë‹¹ ë‹¨ì–´ì˜ ê°€ìš© ê°œìˆ˜ í™•ì¸
        eng_count = eng_word_counter.get(word, 0)
        
        # í•„ìš”í•œ ê°œìˆ˜ë³´ë‹¤ ë¶€ì¡±í•˜ë‹¤ë©´, ë¶€ì¡±í•œ ë§Œí¼ missing_counterì— ì¶”ê°€
        if kor_count > eng_count:
            missing_counter[word] = kor_count - eng_count

    # 5. [ìˆ˜ì •] ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ í˜•ì‹ì— ë§ê²Œ ë°˜í™˜í•©ë‹ˆë‹¤.
    missing_dict = dict(missing_counter)
    status = "all_match" if not missing_dict else "no_match" if len(missing_dict) == len(kor_word_counter) else "partial_match"
    
    return {
        "kor_eng_words": sorted(list(kor_word_counter.keys())), 
        "status": status, 
        "differences": missing_dict # âœ… List ëŒ€ì‹  Dict(ë‹¨ì–´: ë¶€ì¡±í•œ ê°œìˆ˜) ë°˜í™˜
    }

# [ìˆ˜ì •ë¨] differences ì»¬ëŸ¼ì— Dictê°€ ì €ì¥ë˜ë„ë¡ ê¸°ì¡´ êµ¬ì¡° ìœ ì§€
def analyze_english_words_in_korean(df: pd.DataFrame, kor_col: str = "kor_sentence_cleaned", eng_col: str = "eng_sentence_cleaned") -> pd.DataFrame:
    results = [compare_english_words(row.get(kor_col, ""), row.get(eng_col, "")) for _, row in df.iterrows()]
    df_out = df.copy()
    df_out['kor_eng_words'] = [r['kor_eng_words'] for r in results]
    df_out['eng_word_match_status'] = [r['status'] for r in results]
    # âœ… ë°˜í™˜ í˜•ì‹ì´ Listì—ì„œ Dictë¡œ ë³€ê²½ë˜ì—ˆì§€ë§Œ, ê·¸ëŒ€ë¡œ ì»¬ëŸ¼ì— ì €ì¥í•˜ë©´ ë©ë‹ˆë‹¤.
    df_out['eng_word_differences'] = [r['differences'] for r in results]
    return df_out

ALLOWED_CHARS_PATTERN = re.compile(r'[a-zA-Z0-9_ã„±-ã…ã…-ã…£ê°€-í£\s.,!?:;\'\"`~%()â€™â€œâ€Â·/&-]')
def extract_special_symbols(text: str) -> List[str]:
    if not isinstance(text, str): return []
    return list(ALLOWED_CHARS_PATTERN.sub('', text))

def compare_special_symbols(kor_text: str, eng_text: str) -> Dict[str, Any]:
    kor_text, eng_text = unicodedata.normalize("NFKC", kor_text or ""), unicodedata.normalize("NFKC", eng_text or "")
    special_map = {'Â¹':'1','Â²':'2','Â³':'3','â´':'4','âµ':'5','â¶':'6','â·':'7','â¸':'8','â¹':'9','â‚':'1','â‚‚':'2','â‚ƒ':'3','â‚„':'4','â‚…':'5','â‚†':'6','â‚‡':'7','â‚ˆ':'8','â‚‰':'9'}
    k_sym, e_sym, k_num, e_num = Counter(extract_special_symbols(kor_text)), Counter(extract_special_symbols(eng_text)), Counter(re.findall(r'\d',kor_text)), Counter(re.findall(r'\d',eng_text))
    k_rem, e_rem = k_sym.copy(), e_sym.copy()
    common = k_rem & e_rem; k_rem -= common; e_rem -= common
    for sym, count in k_rem.copy().items():
        if (d:=special_map.get(sym)) and e_num[d]>0: m=min(count,e_num[d]); k_rem[sym]-=m; e_num[d]-=m
    for sym, count in e_rem.copy().items():
        if (d:=special_map.get(sym)) and k_num[d]>0: m=min(count,k_num[d]); e_rem[sym]-=m; k_num[d]-=m
    k_rem += Counter(); e_rem += Counter()
    total_initial, total_rem = sum(k_sym.values())+sum(e_sym.values()), sum(k_rem.values())+sum(e_rem.values())
    status = "no_special_symbols" if total_initial == 0 else "all_match" if total_rem == 0 else "partial_match" if total_initial > total_rem else "no_match"
    diffs = {s:{'korean':k_rem[s],'english':e_rem[s]} for s in k_rem.keys()|e_rem.keys() if k_rem[s]>0 or e_rem[s]>0}
    return {'kor_special_symbols': "".join(sorted(k_sym.elements())), 'eng_special_symbols': "".join(sorted(e_sym.elements())), 'symbol_match_status': status, 'symbol_differences': diffs}

def analyze_special_symbols_dataframe(df: pd.DataFrame, kor_col: str = "kor_sentence_cleaned", eng_col: str = "eng_sentence_cleaned") -> pd.DataFrame:
    results = [compare_special_symbols(str(row.get(kor_col, "")), str(row.get(eng_col, ""))) for _, row in df.iterrows()]
    df_out = df.copy()
    df_out['kor_special_symbols'] = [r['kor_special_symbols'] for r in results]
    df_out['eng_special_symbols'] = [r['eng_special_symbols'] for r in results]
    df_out['symbol_match_status'] = [r['symbol_match_status'] for r in results]
    df_out['symbol_differences'] = [r['symbol_differences'] for r in results]
    return df_out

def check_only_eng_korean_sentence(kor_text: str, eng_text: str) -> str:
    """
    í•œ-ì˜ ë¬¸ì¥ ìŒì„ ë¶„ì„í•˜ì—¬ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        - "invalid_input": ì…ë ¥ê°’ì´ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°
        - "empty": ì…ë ¥ ë¬¸ì¥ì´ ëª¨ë‘ ë¹„ì–´ìˆëŠ” ê²½ìš°
        - "no_korean_in_kor": í•œêµ­ì–´ ë¬¸ì¥ì— í•œê¸€ì´ ì—†ëŠ” ê²½ìš°
        - "all_match": ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  ì˜ì–´ì™€ í•œêµ­ì–´ ë¬¸ì¥ì´ ì™„ì „íˆ ê°™ì€ ê²½ìš°
        - "partial_match": ê³µë°± ì œê±° í›„ ì˜ì–´ì™€ í•œêµ­ì–´ ë¬¸ì¥ì´ ê°™ì€ ê²½ìš°
        - "valid_candidate": ìœ„ì˜ ëª¨ë“  ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ”, ìœ íš¨í•œ ë²ˆì—­ ì½”í¼ìŠ¤ í›„ë³´
    """
    # 1. íƒ€ì… ìœ íš¨ì„± ê²€ì‚¬
    if not isinstance(kor_text, str) or not isinstance(eng_text, str):
        return "invalid_input"

    kor_lower, eng_lower = kor_text.lower().strip(), eng_text.lower().strip()

    # 2. ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
    if not kor_lower and not eng_lower:
        return "empty"
    
    # 4. ì™„ì „ ì¼ì¹˜ ê²€ì‚¬
    if kor_lower == eng_lower:
        return "all_match"
    
    # 5. ê³µë°± ì œê±° í›„ ë¶€ë¶„ ì¼ì¹˜ ê²€ì‚¬
    kor_no_space = re.sub(r'\s+', '', kor_lower)
    eng_no_space = re.sub(r'\s+', '', eng_lower)
    if kor_no_space == eng_no_space:
        return "partial_match"
    
    # 3. (í•µì‹¬ ì¶”ê°€) í•œêµ­ì–´ ë¬¸ì¥ì— í•œê¸€ì´ ì—†ëŠ” ê²½ìš° "no_korean_in_kor"ë¡œ ë¶„ë¥˜
    # ì •ê·œì‹: ììŒ/ëª¨ìŒ(ã„±-ã…, ã…-ã…£) ë˜ëŠ” ì™„ì„±í˜• í•œê¸€(ê°€-í£)
    if not re.search(r'[\u3131-\u318E\uAC00-\uD7A3]', kor_lower):
        return "no_korean_in_kor"
        
    # 6. ëª¨ë“  í•„í„°ë§ì„ í†µê³¼í•œ ìœ íš¨í•œ í›„ë³´êµ°
    return "valid_candidate"    

def analyze_only_eng_korean_sentence(df: pd.DataFrame, kor_col: str, eng_col: str) -> pd.DataFrame:
    df_out = df.copy()
    if 'only_eng_korean_sentence' in df_out.columns:
        df_out = df_out.drop(columns=['only_eng_korean_sentence'])
    df_out['only_eng_korean_sentence'] = [check_only_eng_korean_sentence(str(row.get(kor_col, "")), str(row.get(eng_col, ""))) for _, row in df.iterrows()]
    return df_out

def is_numbers_only(text: str) -> bool:
    if not isinstance(text, str) or not text.strip(): return False
    return not bool(re.sub(r'[\d\s,.]', '', text))

def is_symbols_or_single_alphabet_only(text: str) -> bool:
    """
    ë¬¸ì¥ì´ ì•„ë˜ ì¡°ê±´ ì¤‘ í•˜ë‚˜ì— í•´ë‹¹í•˜ë©´ Trueë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    1. ì•ŒíŒŒë²³, í•œê¸€, ìˆ«ì ì—†ì´ 'ë‹¨ í•˜ë‚˜ì˜ ê¸°í˜¸'ë¡œë§Œ êµ¬ì„±ëœ ê²½ìš°
    2. ëŒ€ì†Œë¬¸ì ìƒê´€ì—†ì´ 'ë‹¨ í•˜ë‚˜ì˜ ì•ŒíŒŒë²³'ìœ¼ë¡œë§Œ êµ¬ì„±ëœ ê²½ìš°
    """
    if not isinstance(text, str):
        return False
    
    # 1. ì–‘ìª½ ê³µë°±ì„ ì œê±°í•˜ê³  'nan' ë¬¸ìì—´ ì²˜ë¦¬
    cleaned_text = text.strip().replace('nan', '')
    
    # 2. ì •ë¦¬ëœ ë¬¸ìì—´ì˜ ê¸¸ì´ê°€ ì •í™•íˆ 1ì¸ì§€ í™•ì¸
    if len(cleaned_text) == 1:
        char = cleaned_text[0]
        
        # 3. ì•„ë˜ ë‘ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ True (ì œê±° ëŒ€ìƒ)
        # ì¡°ê±´ A: í•´ë‹¹ ë¬¸ìê°€ ê¸°í˜¸ì¼ ê²½ìš° (ìˆ«ì, í•œê¸€, ì•ŒíŒŒë²³ì´ ì•„ë‹˜)
        is_symbol = not re.match(r'^[a-zA-Z0-9ã„±-ã…ã…-ã…£ê°€-í£]$', char)
        
        # ì¡°ê±´ B: í•´ë‹¹ ë¬¸ìê°€ ì•ŒíŒŒë²³ì¼ ê²½ìš°
        is_alphabet = re.match(r'^[a-zA-Z]$', char)
        
        if is_symbol or is_alphabet:
            return True
            
    # ê¸¸ì´ê°€ 1ì´ ì•„ë‹ˆê±°ë‚˜, ìœ„ ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ ì•Šìœ¼ë©´ False (ì œê±° ëŒ€ìƒ ì•„ë‹˜)
    return False

# ==============================================================================
# 2. ë©”ì¸ ìë™í™” í•¨ìˆ˜ (ìš”êµ¬ì‚¬í•­ì— ë§ì¶° ì¬ì‘ì„±)
# ==============================================================================
def process_and_filter_files(input_dir: str, metadata_output_dir: str, config_path: str = None, config_obj: dict = None):
    # config_objê°€ ì œê³µë˜ë©´ í•´ë‹¹ config ì‚¬ìš©, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ config_pathë¡œ ë¡œë“œ
    if config_obj is not None:
        # config ê°ì²´ê°€ ì§ì ‘ ì œê³µëœ ê²½ìš° ì „ì—­ íŒ¨í„´ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
        global COMMON_NUMBERING_PATTERNS, TEXT_ONLY_NUMBERING_PATTERNS
        COMMON_NUMBERING_PATTERNS = [re.compile(pattern, re.IGNORECASE) for pattern in config_obj['numbering_patterns']['common']]
        TEXT_ONLY_NUMBERING_PATTERNS = [re.compile(pattern) for pattern in config_obj['numbering_patterns']['text_only']]
    elif config_path is not None:
        load_config(config_path)

    input_path = Path(input_dir)
    metadata_path = Path(metadata_output_dir)
    metadata_path.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“‚ ì…ë ¥ í´ë”: '{input_path.resolve()}'")
    print(f"ğŸ“‚ ë©”íƒ€ë°ì´í„° í´ë”: '{metadata_path.resolve()}'")

    files_to_process = list(input_path.glob('*_final_alignment_results.xlsx'))
    if not files_to_process:
        tqdm.write(f"âš ï¸ ì…ë ¥ í´ë” '{input_dir}'ì—ì„œ '_final_alignment_results.xlsx' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    filenames = sorted([p.stem.replace('_final_alignment_results', '') for p in files_to_process])
    tqdm.write(f"ğŸ” ì´ {len(filenames)}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    # --- ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ ì •ì˜ ---
    def _reorder_columns(df: pd.DataFrame) -> pd.DataFrame:
        display_cols = ['kor_sentence_cleaned', 'eng_sentence_cleaned', 'kor_sentence_normalized', 'eng_sentence_normalized']
        status_cols  = ['punct_match_type', 'number_match_status', 'eng_word_match_status', 'symbol_match_status', 'only_eng_korean_sentence']
        def is_analysis_aux(c: str) -> bool: return any(s in c for s in ['_cleaned', '_normalized', '_match_', '_differences', '_numbers', '_punct', '_symbols', '_words'])
        existing_display = [c for c in display_cols if c in df.columns]
        existing_status  = [c for c in status_cols  if c in df.columns]
        original_cols = [c for c in df.columns if not is_analysis_aux(c) and c not in existing_display and c not in existing_status]
        remaining_analysis_cols = [c for c in df.columns if c not in original_cols + existing_display + existing_status]
        final_order = original_cols + existing_display + existing_status + remaining_analysis_cols
        final_order_unique = []
        seen = set()
        for c in final_order:
            if c not in seen:
                final_order_unique.append(c)
                seen.add(c)
        return df[final_order_unique]

    # [ìˆ˜ì •ë¨] ë¬¸ë§¥(context) íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•˜ì—¬ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜
    def _run_full_analysis(df: pd.DataFrame, kor_col: str, eng_col: str, context: str, config_path: str = None) -> pd.DataFrame:
        """
        ë°ì´í„°í”„ë ˆì„ì— ëŒ€í•œ ì „ì²´ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        :param df: ë¶„ì„í•  ë°ì´í„°í”„ë ˆì„
        :param kor_col: í•œêµ­ì–´ ì›ë³¸ ì»¬ëŸ¼ëª…
        :param eng_col: ì˜ì–´ ì›ë³¸ ì»¬ëŸ¼ëª…
        :param context: ë„˜ë²„ë§ ì œê±°ë¥¼ ìœ„í•œ ë¬¸ë§¥ ('text' ë˜ëŠ” 'table')
        :param config_path: config íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        """
        df_copy = df.copy()
        df_copy['kor_sentence_cleaned'] = df_copy[kor_col].astype(str)
        df_copy['eng_sentence_cleaned'] = df_copy[eng_col].astype(str)
        
        for col in ['kor_sentence_cleaned', 'eng_sentence_cleaned']:
            df_copy[col] = (df_copy[col].str.replace(r'^[-\u2010\u2013\u2014\u2212Â·â€¢â—‹:.]\s*', '', regex=True)
                                      .str.replace(r'\s*\*\s*', ' ', regex=True)
                                      .str.strip())
        df_copy['eng_sentence_cleaned'] = df_copy['eng_sentence_cleaned'].str.replace(r'\s*\((IGC|IBC) Code \d+\.\d+\)', '', regex=True)
        
        # âœ… [í•µì‹¬ ë³€ê²½] contextë¥¼ ì „ë‹¬í•˜ì—¬ ë„˜ë²„ë§ ì œê±° í•¨ìˆ˜ í˜¸ì¶œ
        for col in ['kor_sentence_cleaned', 'eng_sentence_cleaned']:
            df_copy[col] = df_copy[col].apply(lambda x: remove_numbering(x, context=context, config_path=config_path))

        df_copy = normalize_quotes_in_dataframe(df_copy, columns=['kor_sentence_cleaned', 'eng_sentence_cleaned'])

        for col in ['kor_sentence_cleaned', 'eng_sentence_cleaned']:
            df_copy[col] = df_copy[col].str.replace(r'-{2,}', '', regex=True).str.strip()

        strip_chars = ':;* '
        for col in ['kor_sentence_cleaned', 'eng_sentence_cleaned']:
            df_copy[col] = df_copy[col].str.strip(strip_chars)

        df_copy['kor_sentence_normalized'] = df_copy['kor_sentence_cleaned'].apply(lambda x: unicodedata.normalize('NFKC', x) if isinstance(x, str) else x)
        df_copy['eng_sentence_normalized'] = df_copy['eng_sentence_cleaned'].apply(lambda x: unicodedata.normalize('NFKC', x) if isinstance(x, str) else x)
        
        number_comparer = IntegratedNumberComparer()
        df_copy = analyze_punctuation_dataframe(df_copy, kor_col='kor_sentence_normalized', eng_col='eng_sentence_normalized')
        df_copy = analyze_dataframe_with_comparer(df_copy, number_comparer, kor_col='kor_sentence_normalized', eng_col='eng_sentence_normalized')
        df_copy = analyze_english_words_in_korean(df_copy, kor_col='kor_sentence_normalized', eng_col='eng_sentence_normalized')
        df_copy = analyze_special_symbols_dataframe(df_copy, kor_col='kor_sentence_normalized', eng_col='eng_sentence_normalized')
        df_copy = analyze_only_eng_korean_sentence(df_copy, kor_col='kor_sentence_normalized', eng_col='eng_sentence_normalized')

        return _reorder_columns(df_copy)

    def _apply_content_filters(df: pd.DataFrame) -> pd.DataFrame:
        kor_col, eng_col = 'kor_sentence_normalized', 'eng_sentence_normalized'
        masks = [
            df.apply(lambda r: is_numbers_only(r[kor_col]) and is_numbers_only(r[eng_col]), axis=1),
            df.apply(lambda r: is_symbols_or_single_alphabet_only(r[kor_col]) or is_symbols_or_single_alphabet_only(r[eng_col]), axis=1)
        ]
        is_to_be_filtered_out = pd.concat(masks, axis=1).any(axis=1)
        return df[~is_to_be_filtered_out]

    def _create_empty_table_dataframe() -> pd.DataFrame:
        """í‘œì¤€í™”ëœ ë¹ˆ Table DataFrameì„ ìƒì„±í•©ë‹ˆë‹¤."""
        # Text ì‹œíŠ¸ì™€ ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì¡°ë¥¼ ê°€ì§„ ë¹ˆ DataFrame
        standard_columns = [
            'korean_sentence', 'english_sentence',
            'kor_sentence_cleaned', 'eng_sentence_cleaned',
            'kor_sentence_normalized', 'eng_sentence_normalized',
            'punct_match_type', 'number_match_status',
            'eng_word_match_status', 'symbol_match_status',
            'only_eng_korean_sentence',
            'kor_punct', 'eng_punct', 'punct_differences',
            'kor_numbers', 'eng_numbers_after_mapping', 'num_differences',
            'kor_eng_words', 'eng_word_differences',
            'kor_special_symbols', 'eng_special_symbols', 'symbol_differences'
        ]
        return pd.DataFrame(columns=standard_columns)

    def _process_table_sheet(df_table_original: pd.DataFrame) -> pd.DataFrame:
        """Table ì‹œíŠ¸ë¥¼ ì²˜ë¦¬í•˜ì—¬ ìµœì¢… DataFrameì„ ë°˜í™˜í•©ë‹ˆë‹¤."""

        # 1. ê¸°ë³¸ ê²€ì¦
        if len(df_table_original) == 0:
            tqdm.write(f"  - âš ï¸ Table ì‹œíŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return _create_empty_table_dataframe()

        tqdm.write(f"  - ğŸ“‹ Table ì‹œíŠ¸ ì»¬ëŸ¼: {list(df_table_original.columns)}")

        # 2. match_source í•„í„°ë§
        if 'match_source' in df_table_original.columns:
            df_filtered = df_table_original[df_table_original['match_source'].notna()].copy()
            tqdm.write(f"  - ğŸ“ match_source í•„í„°ë§ í›„: {len(df_filtered)}í–‰")
        else:
            tqdm.write(f"  - âš ï¸ match_source ì»¬ëŸ¼ ì—†ìŒ, ì „ì²´ ë°ì´í„° ì‚¬ìš©")
            df_filtered = df_table_original.copy()

        # 3. í•„ìˆ˜ ì»¬ëŸ¼ ê²€ì¦
        required_columns = ['korean_sentence', 'english_sentence']
        if not all(col in df_filtered.columns for col in required_columns):
            tqdm.write(f"  - âš ï¸ í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {required_columns}")
            tqdm.write(f"  - ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df_filtered.columns)}")
            return _create_empty_table_dataframe()

        # 4. ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        if len(df_filtered) == 0:
            tqdm.write(f"  - âš ï¸ í•„í„°ë§ í›„ ë°ì´í„° ì—†ìŒ")
            return _create_empty_table_dataframe()

        try:
            # ë¶„ì„ â†’ í•„í„°ë§ â†’ ì¤‘ë³µì œê±°
            analyzed_df = _run_full_analysis(df_filtered, 'korean_sentence', 'english_sentence', context='table', config_path=config_path)
            tqdm.write(f"  - ğŸ”¬ Table ì‹œíŠ¸ ë¶„ì„ ì™„ë£Œ: {len(analyzed_df)}í–‰")

            filtered_df = _apply_content_filters(analyzed_df)
            tqdm.write(f"  - ğŸ§¹ Table ì‹œíŠ¸ ì½˜í…ì¸  í•„í„°ë§ í›„: {len(filtered_df)}í–‰")

            final_df = filtered_df.drop_duplicates(
                subset=['kor_sentence_normalized', 'eng_sentence_normalized'],
                keep='first'
            )
            tqdm.write(f"  - ğŸ”„ Table ì‹œíŠ¸ ì¤‘ë³µ ì œê±° í›„: {len(final_df)}í–‰")
            return final_df

        except Exception as e:
            tqdm.write(f"  - âŒ Table ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return _create_empty_table_dataframe()

    # --- ë©”ì¸ íŒŒì¼ ì²˜ë¦¬ ë£¨í”„ ---
    for filename in tqdm(filenames, desc="ì „ì²´ íŒŒì¼ ì²˜ë¦¬ ì¤‘"):
        input_file = input_path / f'{filename}_final_alignment_results.xlsx'
        tqdm.write(f"\n--- ğŸ“„ '{filename}' íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ---")

        try:
            tqdm.write(f"  - ğŸ“– Excel íŒŒì¼ ì½ê¸° ì‹œì‘...")
            df_text_original = pd.read_excel(input_file, sheet_name='Text', engine='openpyxl')
            tqdm.write(f"  - ğŸ“Š Text ì‹œíŠ¸ ì½ê¸° ì™„ë£Œ: {len(df_text_original)}í–‰")
            df_table_original = pd.read_excel(input_file, sheet_name='Table', engine='openpyxl')
            tqdm.write(f"  - ğŸ“Š Table ì‹œíŠ¸ ì½ê¸° ì™„ë£Œ: {len(df_table_original)}í–‰")

            # --- Text ì‹œíŠ¸ ì²˜ë¦¬ ---
            tqdm.write(f"  - ğŸ” Text ì‹œíŠ¸ ì²˜ë¦¬ ì‹œì‘...")
            df_text_filtered_initial = df_text_original[df_text_original['type'] != 'subordinate_unmatched'].copy()
            tqdm.write(f"  - ğŸ“ Text ì‹œíŠ¸ í•„í„°ë§ í›„: {len(df_text_filtered_initial)}í–‰")

            # âœ… context='text'ë¡œ ë¶„ì„ í•¨ìˆ˜ í˜¸ì¶œ
            analyzed_text_df = _run_full_analysis(df_text_filtered_initial, 'kor_sentence', 'eng_sentence', context='text', config_path=config_path)
            tqdm.write(f"  - ğŸ”¬ Text ì‹œíŠ¸ ë¶„ì„ ì™„ë£Œ: {len(analyzed_text_df)}í–‰")

            final_text_df = _apply_content_filters(analyzed_text_df)
            tqdm.write(f"  - ğŸ§¹ Text ì‹œíŠ¸ ì½˜í…ì¸  í•„í„°ë§ í›„: {len(final_text_df)}í–‰")

            final_text_df = final_text_df.drop_duplicates(
                subset=['kor_sentence_normalized', 'eng_sentence_normalized'],
                keep='first'
            )
            tqdm.write(f"  - ğŸ”„ Text ì‹œíŠ¸ ì¤‘ë³µ ì œê±° í›„: {len(final_text_df)}í–‰")

            # --- Table ì‹œíŠ¸ ì²˜ë¦¬ ---
            tqdm.write(f"  - ğŸ” Table ì‹œíŠ¸ ì²˜ë¦¬ ì‹œì‘...")
            final_table_df = _process_table_sheet(df_table_original)

            # ìµœì¢… ê²°ê³¼ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if len(final_text_df) == 0 and len(final_table_df) == 0:
                tqdm.write(f"  - âš ï¸ ê²½ê³ : ëª¨ë“  ë°ì´í„°ê°€ í•„í„°ë§ë˜ì–´ ì €ì¥í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
                continue

            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            metadata_filename = metadata_path / f"{filename}_filtering_metadata_{timestamp}.xlsx"
            tqdm.write(f"  - ğŸ’¾ Excel íŒŒì¼ ì €ì¥ ì‹œì‘: {metadata_filename.name}")

            with pd.ExcelWriter(metadata_filename, engine='openpyxl') as writer:
                for df in [final_text_df, final_table_df]:
                    for col in [c for c in df.columns if 'score' in c.lower() or 'similarity' in c.lower()]:
                        if col in df.columns:
                            df[col] = df[col].round(3)

                final_text_df.to_excel(writer, sheet_name='Text_Analyzed_Filtered', index=False)
                final_table_df.to_excel(writer, sheet_name='Table_Filtered', index=False)

            tqdm.write(f"  - âœ… ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥ ì™„ë£Œ -> {metadata_filename.name}")

        except Exception as e:
            import traceback
            tqdm.write(f"  - âŒ '{filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            tqdm.write(f"  - ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
            for line in traceback.format_exc().splitlines():
                tqdm.write(f"     {line}")
            continue

if __name__ == '__main__':
    # ì‚¬ìš© ì˜ˆì‹œ:
    INPUT_FOLDER = 'final_aligned_results'  # ì‹¤ì œ ì…ë ¥ í´ë” ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”.
    OUTPUT_FOLDER = 'filtering_metadata'     # ì‹¤ì œ ì¶œë ¥ í´ë” ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”.
    process_and_filter_files(INPUT_FOLDER, OUTPUT_FOLDER)