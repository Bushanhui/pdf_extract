"""
EUAA í…ìŠ¤íŠ¸ ì¡°ê°ì„ ì˜ë¯¸ì ìœ¼ë¡œ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ëŠ” ì‹œìŠ¤í…œ

JSON í˜•ì‹ì˜ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ LLMì„ í†µí•´ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
ë°°ì¹˜ ì²˜ë¦¬ ë° ì¤‘ë³µ ì œê±°ë¥¼ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€ëŸ‰ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""

import os
import json
import sys
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
import time
import google.generativeai as genai
from dotenv import load_dotenv
from langfuse.decorators import observe

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


class Logger:
    """ë¡œê¹… ì‹œìŠ¤í…œ ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)

        # ì„¸ì…˜ë³„ ë¡œê·¸ íŒŒì¼
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session_log = self.log_dir / f"sentence_compose_{timestamp}.log"

        # ë¡œê±° ì„¤ì •
        self.logger = logging.getLogger('SentenceComposer')
        self.logger.setLevel(logging.INFO)

        # íŒŒì¼ í•¸ë“¤ëŸ¬
        file_handler = logging.FileHandler(self.session_log, encoding='utf-8')
        file_handler.setLevel(logging.INFO)

        # ì½˜ì†” í•¸ë“¤ëŸ¬
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)

        # í¬ë§·í„°
        formatter = logging.Formatter('[%(asctime)s] %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)

        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)

    def info(self, message: str):
        """ì •ë³´ ë©”ì‹œì§€ ë¡œê¹…"""
        self.logger.info(message)

    def warning(self, message: str):
        """ê²½ê³  ë©”ì‹œì§€ ë¡œê¹…"""
        self.logger.warning(f"âš ï¸  {message}")

    def error(self, message: str):
        """ì˜¤ë¥˜ ë©”ì‹œì§€ ë¡œê¹…"""
        self.logger.error(f"âŒ {message}")

    def success(self, message: str):
        """ì„±ê³µ ë©”ì‹œì§€ ë¡œê¹…"""
        self.logger.info(f"âœ“ {message}")


class FileProcessor:
    """sentences í´ë”ì˜ JSON íŒŒì¼ë“¤ì„ ìŠ¤ìº”í•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, sentences_dir: str = "sentences"):
        self.sentences_dir = Path(sentences_dir)
        if not self.sentences_dir.exists():
            raise FileNotFoundError(f"sentences í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {sentences_dir}")

    def scan_input_files(self) -> List[Dict[str, Any]]:
        """llm_input_*.json íŒŒì¼ë“¤ì„ ìŠ¤ìº”í•˜ì—¬ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        pattern = "llm_input_*ë¶€_*.json"
        input_files = list(self.sentences_dir.glob(pattern))

        file_info = []
        for file_path in sorted(input_files):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                info = {
                    "file_path": str(file_path),
                    "filename": file_path.name,
                    "object_count": len(data),
                    "file_size": file_path.stat().st_size,
                    "part_number": self._extract_part_number(file_path.name),
                    "language": self._extract_language(file_path.name),
                    "output_path": self._get_output_path(file_path.name)
                }
                file_info.append(info)

            except Exception as e:
                print(f"íŒŒì¼ ìŠ¤ìº” ì‹¤íŒ¨: {file_path.name} - {e}")

        return file_info

    def _extract_part_number(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ë¶€ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: '1ë¶€', '2ë¶€')"""
        import re
        match = re.search(r'(\d+ë¶€)', filename)
        return match.group(1) if match else "ì•Œìˆ˜ì—†ìŒ"

    def _extract_language(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ì–¸ì–´ ì¶”ì¶œ (en ë˜ëŠ” kr)"""
        if '_en.json' in filename:
            return 'en'
        elif '_kr.json' in filename:
            return 'kr'
        return 'unknown'

    def _get_output_path(self, input_filename: str) -> str:
        """ì…ë ¥ íŒŒì¼ëª…ì—ì„œ ì¶œë ¥ íŒŒì¼ëª… ìƒì„±"""
        output_filename = input_filename.replace('llm_input_', 'llm_output_')
        return str(self.sentences_dir / output_filename)


class BatchProcessor:
    """JSON ë°°ì—´ì„ ê²¹ì¹¨ì„ í¬í•¨í•œ ë°°ì¹˜ë¡œ ë¶„í• í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, batch_size: int = 250, overlap_size: int = 10):
        self.batch_size = batch_size
        self.overlap_size = overlap_size

    def split_into_batches(self, data: List[Dict]) -> List[Dict[str, Any]]:
        """
        JSON ë°°ì—´ì„ ê²¹ì¹¨ì„ í¬í•¨í•œ ë°°ì¹˜ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

        Args:
            data: JSON ê°ì²´ ë°°ì—´

        Returns:
            ë°°ì¹˜ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        if not data:
            return []

        batches = []
        total_objects = len(data)
        batch_num = 1
        start_idx = 0

        while start_idx < total_objects:
            # ë°°ì¹˜ ë ì¸ë±ìŠ¤ ê³„ì‚°
            end_idx = min(start_idx + self.batch_size, total_objects)

            # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
            batch_data = data[start_idx:end_idx]

            batch_info = {
                "batch_number": batch_num,
                "start_index": start_idx,
                "end_index": end_idx - 1,
                "object_count": len(batch_data),
                "data": batch_data,
                "is_last_batch": end_idx >= total_objects
            }

            batches.append(batch_info)

            # ë‹¤ìŒ ë°°ì¹˜ ì‹œì‘ ì¸ë±ìŠ¤ ê³„ì‚° (ê²¹ì¹¨ ê³ ë ¤)
            if end_idx >= total_objects:
                break

            start_idx = end_idx - self.overlap_size
            batch_num += 1

        return batches


class APIKeyManager:
    """API í‚¤ ê´€ë¦¬ í´ë˜ìŠ¤ (ì¼ì¼ í• ë‹¹ëŸ‰ ì¶”ì  í¬í•¨)"""

    def __init__(self):
        # API í‚¤ë“¤ ë¡œë“œ (BACKUP1, BACKUP2, ... í˜•ì‹)
        self.api_keys = []

        # ë©”ì¸ í‚¤ ì¶”ê°€
        primary_key = os.getenv("GOOGLE_API_KEY")
        if primary_key:
            self.api_keys.append(primary_key)

        # ë°±ì—… í‚¤ë“¤ ì¶”ê°€ (GOOGLE_API_KEY_BACKUP_1, GOOGLE_API_KEY_BACKUP_2, ...)
        backup_index = 1
        while True:
            backup_key = os.getenv(f"GOOGLE_API_KEY_BACKUP_{backup_index}")
            if backup_key:
                self.api_keys.append(backup_key)
                backup_index += 1
            else:
                break

        if not self.api_keys:
            raise ValueError("API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # API í‚¤ ìƒíƒœ ê´€ë¦¬
        self.usage_file = "api_key_usage.json"
        self._load_key_usage()  # JSONì—ì„œ í‚¤ ìƒíƒœ ë¡œë“œ
        self.current_key_index = self._get_available_key_index()
        self.current_api_key = self.api_keys[self.current_key_index]

        # genai ì´ˆê¸°í™”
        genai.configure(api_key=self.current_api_key)

        # API í‚¤ ìƒíƒœ ì¶œë ¥
        print(f"âœ… ì„¤ì •ëœ API í‚¤ ê°œìˆ˜: {len(self.api_keys)}ê°œ")
        for i, key in enumerate(self.api_keys):
            key_type = "ë©”ì¸" if i == 0 else f"ë°±ì—…{i}"
            key_id = f"***{key[-4:]}"
            key_info = self.key_usage.get(key_id, {'status': 'available'})
            status_emoji = "ğŸŸ¢" if key_info['status'] == 'available' else "ğŸ”´"
            current_mark = " â†í˜„ì¬ì„ íƒ" if i == self.current_key_index else ""
            print(f"  {key_type} API í‚¤: {key_id} {status_emoji}{key_info['status']}{current_mark}")

    def _load_key_usage(self) -> None:
        """JSON íŒŒì¼ì—ì„œ API í‚¤ ì‚¬ìš© ìƒíƒœë¥¼ ë¡œë“œí•˜ê³  24ì‹œê°„ ê²½ê³¼ í‚¤ëŠ” ìë™ìœ¼ë¡œ ë¦¬ì…‹"""
        try:
            if not Path(self.usage_file).exists():
                self.key_usage = self._initialize_key_usage()
                self._save_key_usage()
                print(f"ğŸ“ API í‚¤ ì‚¬ìš©ëŸ‰ ì¶”ì  íŒŒì¼ ìƒì„±: {self.usage_file}")
                return

            with open(self.usage_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.key_usage = data.get('keys', {})

            # 24ì‹œê°„ ê²½ê³¼ í‚¤ë“¤ì„ ìë™ìœ¼ë¡œ availableë¡œ ë¦¬ì…‹
            current_time = datetime.now()
            reset_count = 0

            for key_id, key_info in self.key_usage.items():
                if key_info.get('reset_time'):
                    reset_time = datetime.fromisoformat(key_info['reset_time'])
                    if current_time >= reset_time and key_info['status'] == 'exhausted':
                        key_info['status'] = 'available'
                        key_info['first_used'] = None
                        key_info['reset_time'] = None
                        reset_count += 1

            if reset_count > 0:
                print(f"ğŸ”„ {reset_count}ê°œ API í‚¤ê°€ 24ì‹œê°„ ê²½ê³¼ë¡œ ì‚¬ìš© ê°€ëŠ¥ ìƒíƒœë¡œ ë¦¬ì…‹ë¨")
                self._save_key_usage()

            print(f"âœ… API í‚¤ ì‚¬ìš©ëŸ‰ ìƒíƒœ ë¡œë“œ ì™„ë£Œ: {len(self.key_usage)}ê°œ í‚¤")

        except Exception as e:
            print(f"âš ï¸ API í‚¤ ì‚¬ìš©ëŸ‰ ë¡œë“œ ì‹¤íŒ¨, ì´ˆê¸°í™”í•©ë‹ˆë‹¤: {e}")
            self.key_usage = self._initialize_key_usage()
            self._save_key_usage()

    def _initialize_key_usage(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  API í‚¤ë¥¼ ì‚¬ìš© ê°€ëŠ¥ ìƒíƒœë¡œ ì´ˆê¸°í™”"""
        usage = {}
        for api_key in self.api_keys:
            key_id = f"***{api_key[-4:]}"
            usage[key_id] = {
                'first_used': None,
                'status': 'available',
                'reset_time': None
            }
        return usage

    def _save_key_usage(self) -> None:
        """í˜„ì¬ API í‚¤ ì‚¬ìš© ìƒíƒœë¥¼ JSON íŒŒì¼ì— ì €ì¥"""
        try:
            data = {
                'keys': self.key_usage,
                'last_updated': datetime.now().isoformat()
            }

            with open(self.usage_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

        except Exception as e:
            print(f"âš ï¸ API í‚¤ ì‚¬ìš©ëŸ‰ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _get_available_key_index(self) -> int:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì²« ë²ˆì§¸ API í‚¤ì˜ ì¸ë±ìŠ¤ë¥¼ ë°˜í™˜"""
        for i, api_key in enumerate(self.api_keys):
            key_id = f"***{api_key[-4:]}"
            key_info = self.key_usage.get(key_id, {'status': 'available'})

            if key_info['status'] == 'available':
                print(f"ğŸ”‘ ì‚¬ìš© ê°€ëŠ¥í•œ API í‚¤ ì„ íƒ: {key_id} (ì¸ë±ìŠ¤ {i})")
                return i

        # ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í‚¤ ì‚¬ìš©
        print(f"âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ í‚¤ê°€ ì—†ì–´ ë©”ì¸ í‚¤ë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤")
        return 0

    def _mark_key_exhausted(self, key_index: int) -> None:
        """ì§€ì •ëœ í‚¤ë¥¼ í• ë‹¹ëŸ‰ ì†Œì§„ ìƒíƒœë¡œ ë§ˆí‚¹í•˜ê³  24ì‹œê°„ í›„ ë¦¬ì…‹ ì‹œê°„ì„ ì„¤ì •"""
        if key_index >= len(self.api_keys):
            return

        api_key = self.api_keys[key_index]
        key_id = f"***{api_key[-4:]}"
        current_time = datetime.now()

        if key_id not in self.key_usage:
            self.key_usage[key_id] = {'first_used': None, 'status': 'available', 'reset_time': None}

        # ì²« ì‚¬ìš©ì¸ ê²½ìš° ì‹œì‘ ì‹œê°„ ê¸°ë¡
        if not self.key_usage[key_id]['first_used']:
            self.key_usage[key_id]['first_used'] = current_time.isoformat()

        # ìƒíƒœë¥¼ exhaustedë¡œ ë³€ê²½í•˜ê³  24ì‹œê°„ í›„ ë¦¬ì…‹ ì‹œê°„ ì„¤ì •
        self.key_usage[key_id]['status'] = 'exhausted'
        self.key_usage[key_id]['reset_time'] = (current_time + timedelta(hours=24)).isoformat()

        self._save_key_usage()

        reset_time_str = (current_time + timedelta(hours=24)).strftime('%Y-%m-%d %H:%M:%S')
        print(f"â° API í‚¤ {key_id} í• ë‹¹ëŸ‰ ì†Œì§„ìœ¼ë¡œ ë§ˆí‚¹, ë¦¬ì…‹ ì‹œê°„: {reset_time_str}")

    def switch_to_next_key(self) -> bool:
        """ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜ (í˜„ì¬ í‚¤ë¥¼ exhaustedë¡œ ë§ˆí‚¹)"""
        # í˜„ì¬ í‚¤ë¥¼ í• ë‹¹ëŸ‰ ì†Œì§„ ìƒíƒœë¡œ ë§ˆí‚¹
        self._mark_key_exhausted(self.current_key_index)

        if self.current_key_index < len(self.api_keys) - 1:
            self.current_key_index += 1
            self.current_api_key = self.api_keys[self.current_key_index]

            key_type = "ë©”ì¸" if self.current_key_index == 0 else f"ë°±ì—… í‚¤ {self.current_key_index}"
            print(f"ğŸ”„ ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜: {key_type} (***...{self.current_api_key[-4:]})")

            genai.configure(api_key=self.current_api_key)
            return True

        print(f"âŒ ëª¨ë“  API í‚¤ ({len(self.api_keys)}ê°œ) ì‚¬ìš© ì™„ë£Œ")
        return False


class SentenceComposer:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, logger: Logger):
        self.api_manager = APIKeyManager()
        self.logger = logger
        self.session_id = f"sentence_compose_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        # ë¬¸ì¥ êµ¬ì„± ì „ìš© í”„ë¡¬í”„íŠ¸ (P íƒœê·¸ë§Œ ì²˜ë¦¬)
        self.compose_prompt = """ë‹¹ì‹ ì€ ê¸°ê³„ ë²ˆì—­ í•™ìŠµ ë°ì´í„°ì…‹ì„ ë§Œë“¤ê¸° ìœ„í•´, ì¤„ë°”ê¿ˆìœ¼ë¡œ ë‚˜ë‰œ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ì™„ì „í•œ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¬êµ¬ì„±í•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**# ì…ë ¥ í˜•ì‹**
ì¤„ë°”ê¿ˆ(`\n`)ìœ¼ë¡œ êµ¬ë¶„ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤. ê° ì¤„ì€ ë³¸ë¬¸(P) í…ìŠ¤íŠ¸ì˜ ì¼ë¶€ì…ë‹ˆë‹¤.

**# ìµœì¢… ëª©í‘œ**
ì…ë ¥ëœ í…ìŠ¤íŠ¸ ì¡°ê°ë“¤ì„ í•©ì³ì„œ ì˜ë¯¸ìƒ ì™„ì „í•œ ë¬¸ì¥ë“¤ë¡œ êµ¬ì„±ëœ JSON ë°°ì—´ì„ ì¶œë ¥í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

**# ì²˜ë¦¬ ê·œì¹™**

1. **ë¬¸ì¥ ë³‘í•© ê·œì¹™**:
   - ì¤„ë°”ê¿ˆìœ¼ë¡œ ë‚˜ë‰œ í…ìŠ¤íŠ¸ë¥¼ ìˆœì„œëŒ€ë¡œ ì½ìœ¼ë©´ì„œ, ì™„ì „í•œ ë¬¸ì¥ì´ ë  ë•Œê¹Œì§€ ì´ì–´ ë¶™ì…ë‹ˆë‹¤.
   - ì™„ì „í•œ ë¬¸ì¥ ê¸°ì¤€:
     * ë§ˆì¹¨í‘œ(.), ë¬¼ìŒí‘œ(?), ëŠë‚Œí‘œ(!) ë“±ìœ¼ë¡œ ëë‚˜ê³  ì˜ë¯¸ê°€ ì™„ê²°ëœ ê²½ìš°
     * ê´„í˜¸ ì§ì´ ë§ëŠ” ê²½ìš°: ì—¬ëŠ” ê´„í˜¸(`[`, `(`)ê°€ ìˆìœ¼ë©´ ë‹«ëŠ” ê´„í˜¸(`]`, `)`)ê¹Œì§€ ì´ì–´ ë¶™ì—¬ì•¼ í•¨
   - ì´ë¯¸ ì™„ì „í•œ ë¬¸ì¥ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì¶œë ¥ ë°°ì—´ì— ì¶”ê°€í•©ë‹ˆë‹¤.

2. **ë¬¸ì¥ ë¶„ë¦¬ ê·œì¹™**:
   - í•˜ë‚˜ì˜ ì¤„ì— ì—¬ëŸ¬ ê°œì˜ ì™„ì „í•œ ë¬¸ì¥ì´ ìˆìœ¼ë©´, ê°ê°ì„ ë³„ê°œì˜ JSON ê°ì²´ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
   - ê° JSON ê°ì²´ëŠ” ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ì™„ì „í•œ ë¬¸ì¥ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

3. **í…ìŠ¤íŠ¸ ì •ë¦¬**:
   - ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±° (ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì€ í•˜ë‚˜ë¡œ)
   - ê´„í˜¸ ì•ë’¤ ê³µë°± ì •ë¦¬: `( text )` â†’ `(text)`

**# ì¶œë ¥ í˜•ì‹**
JSON ë°°ì—´ë¡œ ì¶œë ¥í•˜ë©°, ê° ê°ì²´ëŠ” ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¦…ë‹ˆë‹¤:
```
{"text": "ì™„ì „í•œ ë¬¸ì¥", "source_type": "P"}
```
- `text`: ì˜ë¯¸ì ìœ¼ë¡œ ì™„ì „í•œ í•˜ë‚˜ì˜ ë¬¸ì¥
- `source_type`: í•­ìƒ "P" (ë³¸ë¬¸)

**# ì˜ˆì‹œ**

ì…ë ¥ í…ìŠ¤íŠ¸:
```
ë³´í˜¸ì ë¯¸ë™ë°˜ ì•„ë™
ë£©ì…ˆë¶€ë¥´í¬ í–‰ì •ë²•ì›ì€ A ì™€ S ì— ëŒ€í•œ ìœ ëŸ½ì—°í•©ì‚¬ë²•ì¬íŒì†Œ (CJEU) íŒê²° ( ì œ C-550/16 í˜¸ ) ì„ ê³ ë ¤í•˜ì—¬ ë¯¸ì„±ë…„ìê°€
ë³´í˜¸ì ë¯¸ë™ë°˜ ì•„ë™ìœ¼ë¡œ ê°„ì£¼ë˜ëŠ” ì¡°ê±´ì„ ë¶„ì„í•˜ì˜€ë‹¤ .
```

ì¶œë ¥ JSON:
```json
[
  {"text": "ë³´í˜¸ì ë¯¸ë™ë°˜ ì•„ë™", "source_type": "P"},
  {"text": "ë£©ì…ˆë¶€ë¥´í¬ í–‰ì •ë²•ì›ì€ Aì™€ Sì— ëŒ€í•œ ìœ ëŸ½ì—°í•©ì‚¬ë²•ì¬íŒì†Œ(CJEU) íŒê²°(ì œC-550/16í˜¸)ì„ ê³ ë ¤í•˜ì—¬ ë¯¸ì„±ë…„ìê°€ ë³´í˜¸ì ë¯¸ë™ë°˜ ì•„ë™ìœ¼ë¡œ ê°„ì£¼ë˜ëŠ” ì¡°ê±´ì„ ë¶„ì„í•˜ì˜€ë‹¤.", "source_type": "P"}
]
```

**ë°˜ë“œì‹œ JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ê³ , ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.**"""

    def split_into_sections(self, input_data: List[Dict]) -> List[Dict[str, Any]]:
        """ì…ë ¥ ë°ì´í„°ë¥¼ H1/H2/H3 ê¸°ì¤€ìœ¼ë¡œ ì„¹ì…˜ìœ¼ë¡œ ë¶„í• 

        Args:
            input_data: JSON ê°ì²´ ë°°ì—´ (id, text, source_type í¬í•¨)

        Returns:
            ì„¹ì…˜ ë¦¬ìŠ¤íŠ¸. ê° ì„¹ì…˜ì€ {'headers': [H1/H2/H3 í•­ëª©ë“¤], 'p_items': [P í•­ëª©ë“¤]} í˜•íƒœ
        """
        if not input_data:
            return []

        sections = []
        current_section = {'headers': [], 'p_items': []}

        for item in input_data:
            if item.get('source_type') in ['H1', 'H2', 'H3']:
                # ì´ì „ ì„¹ì…˜ì´ ìˆìœ¼ë©´ ì €ì¥
                if current_section['p_items'] or current_section['headers']:
                    sections.append(current_section)
                # ìƒˆ ì„¹ì…˜ ì‹œì‘
                current_section = {
                    'headers': [item],
                    'p_items': []
                }
            else:  # P
                current_section['p_items'].append(item)

        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
        if current_section['p_items'] or current_section['headers']:
            sections.append(current_section)

        return sections

    def process_section_p_batch(self, p_items: List[Dict], section_number: int, part_info: str, batch_size: int = 100) -> List[Dict]:
        """ì„¹ì…˜ ë‚´ P í•­ëª©ë“¤ì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ LLM ì²˜ë¦¬

        Args:
            p_items: ì²˜ë¦¬í•  P í•­ëª© ë¦¬ìŠ¤íŠ¸
            section_number: ì„¹ì…˜ ë²ˆí˜¸ (ë¡œê¹…ìš©)
            part_info: íŒŒíŠ¸ ì •ë³´ (ì˜ˆ: "1ë¶€_kr")
            batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’ 100)

        Returns:
            LLM ì²˜ë¦¬ëœ P í•­ëª© ë¦¬ìŠ¤íŠ¸
        """
        if not p_items:
            return []

        all_composed_p = []
        total_p_count = len(p_items)
        batch_num = 1
        start_idx = 0

        self.logger.info(f"  ì„¹ì…˜ {section_number}: {total_p_count}ê°œ P í•­ëª©ì„ {batch_size}ê°œì”© ë°°ì¹˜ ì²˜ë¦¬")

        while start_idx < total_p_count:
            # ë°°ì¹˜ ë ì¸ë±ìŠ¤ ê³„ì‚°
            end_idx = min(start_idx + batch_size, total_p_count)
            batch_data = p_items[start_idx:end_idx]

            self.logger.info(f"    ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘... ({len(batch_data)}ê°œ P í•­ëª©)")

            # LLM í˜¸ì¶œ
            result = self.process_batch(batch_data, batch_num, part_info)

            if result['success']:
                all_composed_p.extend(result['composed_data'])
                self.logger.info(f"    âœ“ ë°°ì¹˜ {batch_num} ì™„ë£Œ: {len(batch_data)}ê°œ â†’ {len(result['composed_data'])}ê°œ")
            else:
                self.logger.error(f"    âœ— ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                # ì‹¤íŒ¨í•œ ê²½ìš° ì›ë³¸ ë°ì´í„°ë¥¼ ê·¸ëŒ€ë¡œ ì¶”ê°€
                all_composed_p.extend(batch_data)

            start_idx = end_idx
            batch_num += 1

        return all_composed_p

    def process_file(self, input_data: List[Dict], part_info: str, batch_size: int = 100) -> Tuple[List[Dict], Dict[str, Any], List[Dict]]:
        """íŒŒì¼ ì „ì²´ë¥¼ ì„¹ì…˜ë³„ë¡œ ì²˜ë¦¬

        Args:
            input_data: ì…ë ¥ JSON ë°ì´í„° (ì „ì²´ íŒŒì¼)
            part_info: íŒŒíŠ¸ ì •ë³´ (ì˜ˆ: "1ë¶€_kr")
            batch_size: P ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸ê°’ 100)

        Returns:
            (ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬, H1/H2/H3 í—¤ë” ë¦¬ìŠ¤íŠ¸)
        """
        # 1. ì„¹ì…˜ìœ¼ë¡œ ë¶„í• 
        self.logger.info(f"ì„¹ì…˜ ë¶„í•  ì¤‘...")
        sections = self.split_into_sections(input_data)
        self.logger.info(f"ì´ {len(sections)}ê°œ ì„¹ì…˜ìœ¼ë¡œ ë¶„í• ë¨")

        # 2. ê° ì„¹ì…˜ ì²˜ë¦¬
        final_results = []
        all_headers = []  # H1/H2/H3 í—¤ë”ë§Œ ë³„ë„ë¡œ ìˆ˜ì§‘
        total_h1_count = 0
        total_h2_count = 0
        total_h3_count = 0
        total_input_p_count = 0
        total_output_p_count = 0

        for idx, section in enumerate(sections, 1):
            self.logger.info(f"\nì„¹ì…˜ {idx}/{len(sections)} ì²˜ë¦¬ ì¤‘...")

            # H1/H2/H3 ì§ì ‘ ì¶”ê°€ (LLM ì²˜ë¦¬ ì—†ìŒ)
            headers = section.get('headers', [])
            for header in headers:
                final_results.append(header)
                all_headers.append(header)  # anchorìš© ë³„ë„ ìˆ˜ì§‘
                if header.get('source_type') == 'H1':
                    total_h1_count += 1
                elif header.get('source_type') == 'H2':
                    total_h2_count += 1
                elif header.get('source_type') == 'H3':
                    total_h3_count += 1

            if headers:
                self.logger.info(f"  H1/H2/H3 í—¤ë”: {len(headers)}ê°œ ì§ì ‘ ì¶”ê°€ (LLM ì²˜ë¦¬ ì•ˆ í•¨)")

            # P í•­ëª© ì²˜ë¦¬
            p_items = section.get('p_items', [])
            if p_items:
                total_input_p_count += len(p_items)

                # Pê°€ 1ê°œë§Œ ìˆìœ¼ë©´ LLM ì²˜ë¦¬ ì—†ì´ ì§ì ‘ ì¶”ê°€
                if len(p_items) == 1:
                    final_results.extend(p_items)
                    total_output_p_count += 1
                    self.logger.info(f"  P í•­ëª©: 1ê°œ ì§ì ‘ ì¶”ê°€ (LLM ì²˜ë¦¬ ì•ˆ í•¨)")
                else:
                    # Pê°€ 2ê°œ ì´ìƒì´ë©´ ë°°ì¹˜ ì²˜ë¦¬
                    composed_p = self.process_section_p_batch(p_items, idx, part_info, batch_size)
                    final_results.extend(composed_p)
                    total_output_p_count += len(composed_p)

        # 3. ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = {
            "total_input_count": len(input_data),
            "total_sections": len(sections),
            "h1_count": total_h1_count,
            "h2_count": total_h2_count,
            "h3_count": total_h3_count,
            "input_p_count": total_input_p_count,
            "output_p_count": total_output_p_count,
            "overall_compression_ratio": total_input_p_count / total_output_p_count if total_output_p_count > 0 else 0
        }

        self.logger.info(f"\nì²˜ë¦¬ ì™„ë£Œ!")
        self.logger.info(f"  ì „ì²´ ì…ë ¥: {len(input_data):,}ê°œ")
        self.logger.info(f"  ì „ì²´ ì¶œë ¥: {len(final_results):,}ê°œ")
        self.logger.info(f"  H1 í—¤ë”: {total_h1_count}ê°œ (100% ë³´ì¡´)")
        self.logger.info(f"  H2 í—¤ë”: {total_h2_count}ê°œ (100% ë³´ì¡´)")
        self.logger.info(f"  H3 í—¤ë”: {total_h3_count}ê°œ (100% ë³´ì¡´)")
        self.logger.info(f"  P ì••ì¶•ë¥ : {metadata['overall_compression_ratio']:.1f}ë°° ({total_input_p_count} â†’ {total_output_p_count})")

        return final_results, metadata, all_headers

    def process_batch(self, batch_data: List[Dict], batch_number: int, part_info: str) -> Dict[str, Any]:
        """
        ë°°ì¹˜ ë°ì´í„°ë¥¼ LLMìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë¬¸ì¥ì„ êµ¬ì„±í•©ë‹ˆë‹¤.

        Args:
            batch_data: ì²˜ë¦¬í•  ë°°ì¹˜ ë°ì´í„° (P í•­ëª©ë“¤)
            batch_number: ë°°ì¹˜ ë²ˆí˜¸
            part_info: íŒŒíŠ¸ ì •ë³´ (ì˜ˆ: "1ë¶€_kr")

        Returns:
            ì²˜ë¦¬ ê²°ê³¼
        """
        try:
            self.logger.info(f"ë°°ì¹˜ {batch_number} ì²˜ë¦¬ ì‹œì‘ ({len(batch_data)}ê°œ P í•­ëª©)")

            # P í•­ëª©ë“¤ì˜ textë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì´ì–´ë¶™ì´ê¸°
            input_text = "\n".join([item['text'] for item in batch_data])

            # LLM í˜¸ì¶œ
            start_time = time.time()
            response = self._call_llm_simple(input_text)

            if not response:
                self.logger.error(f"ë°°ì¹˜ {batch_number}: LLM ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
                return {
                    "success": False,
                    "error": "LLM ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤",
                    "batch_number": batch_number
                }

            # ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ì‹œë„
            composed_data = self._extract_json_from_response(response)

            if not composed_data:
                self.logger.error(f"ë°°ì¹˜ {batch_number}: ìœ íš¨í•œ JSONì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return {
                    "success": False,
                    "error": "LLM ì‘ë‹µì—ì„œ ìœ íš¨í•œ JSONì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "batch_number": batch_number,
                    "raw_response": response
                }

            processing_time = time.time() - start_time

            # ì••ì¶•ë¥  ê³„ì‚°
            compression_ratio = len(batch_data) / len(composed_data) if len(composed_data) > 0 else 0

            # ë¡œê·¸ ì¶œë ¥
            self.logger.info(f"â”œâ”€ ì••ì¶•ë¥ : {compression_ratio:.1f}ë°° ({len(batch_data)}ê°œ P â†’ {len(composed_data)}ê°œ ë¬¸ì¥)")
            self.logger.info(f"â””â”€ ì²˜ë¦¬ì‹œê°„: {processing_time:.1f}ì´ˆ")

            return {
                "success": True,
                "batch_number": batch_number,
                "input_count": len(batch_data),
                "output_count": len(composed_data),
                "compression_ratio": compression_ratio,
                "composed_data": composed_data,
                "processing_time": processing_time
            }

        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ {batch_number} ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "batch_number": batch_number
            }

    @observe(name="gemini_sentence_composition")
    def _call_llm_simple(self, input_text: str) -> Optional[str]:
        """ê°„ë‹¨í•œ LLM í˜¸ì¶œ (í…ìŠ¤íŠ¸ ì…ë ¥ë°›ì•„ JSON ì‘ë‹µ ë°˜í™˜)"""
        from langfuse.decorators import langfuse_context

        max_retries = len(self.api_manager.api_keys)

        # JSON Schema ì •ì˜
        response_schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "text": {
                        "type": "string",
                        "description": "êµ¬ì„±ëœ ì™„ì „í•œ ë¬¸ì¥"
                    },
                    "source_type": {
                        "type": "string",
                        "description": "ì›ë³¸ ì¶œì²˜ ìœ í˜•: í•­ìƒ P"
                    }
                },
                "required": ["text", "source_type"]
            }
        }

        # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        full_prompt = f"{self.compose_prompt}\n\nì…ë ¥ í…ìŠ¤íŠ¸:\n{input_text}"

        # Langfuseì— í”„ë¡¬í”„íŠ¸ì™€ ì…ë ¥ ë©”íƒ€ë°ì´í„° ê¸°ë¡
        langfuse_context.update_current_observation(
            input=full_prompt,
            metadata={
                "model": "gemini-2.5-flash",
                "temperature": 0.7,
                "max_output_tokens": 65536,
                "input_text_length": len(input_text),
                "input_line_count": input_text.count('\n') + 1
            }
        )

        for attempt in range(max_retries):
            try:
                llm_start_time = time.time()

                # ëª¨ë¸ ì´ˆê¸°í™” (structured output ë° temperature 0.7 ì„¤ì •)
                model = genai.GenerativeModel(
                    "gemini-2.5-flash",
                    generation_config={
                        "response_mime_type": "application/json",
                        "response_schema": response_schema,
                        "temperature": 0.7,
                        "max_output_tokens": 65536
                    }
                )

                # LLM í˜¸ì¶œ
                response = model.generate_content(full_prompt)

                llm_duration = time.time() - llm_start_time

                if response and response.text:
                    # Langfuseì— ì‘ë‹µê³¼ ì†Œìš”ì‹œê°„ ê¸°ë¡
                    langfuse_context.update_current_observation(
                        output=response.text,
                        metadata={
                            "llm_duration_seconds": round(llm_duration, 2),
                            "output_length": len(response.text),
                            "attempt": attempt + 1,
                            "api_key_index": self.api_manager.current_key_index
                        }
                    )
                    return response.text

                return None

            except Exception as e:
                error_message = str(e)
                llm_duration = time.time() - llm_start_time if 'llm_start_time' in locals() else 0

                # Langfuseì— ì—ëŸ¬ ê¸°ë¡
                langfuse_context.update_current_observation(
                    metadata={
                        "error": error_message,
                        "attempt": attempt + 1,
                        "llm_duration_seconds": round(llm_duration, 2)
                    }
                )

                self.logger.warning(f"API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {error_message}")

                # í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€
                if self._is_daily_quota_exceeded(e):
                    self.logger.warning(f"ğŸ“Š ì¼ë³„ í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°ì§€")
                    # ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜
                    if attempt < max_retries - 1 and self.api_manager.switch_to_next_key():
                        self.logger.info(f"ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜í•˜ì—¬ ì¬ì‹œë„...")
                        continue
                else:
                    # ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜ (ì¼ë°˜ ì—ëŸ¬)
                    if attempt < max_retries - 1 and self.api_manager.switch_to_next_key():
                        self.logger.info(f"ë‹¤ìŒ API í‚¤ë¡œ ì „í™˜í•˜ì—¬ ì¬ì‹œë„...")
                        continue

                # ëª¨ë“  í‚¤ ì‹œë„ ì™„ë£Œ
                if attempt == max_retries - 1:
                    self.logger.error(f"ëª¨ë“  API í‚¤ ì‹œë„ ì™„ë£Œ. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {error_message}")
                    return None

        return None

    def _is_daily_quota_exceeded(self, error: Exception) -> bool:
        """ì¼ë³„ í• ë‹¹ëŸ‰ ì´ˆê³¼ ì—ëŸ¬ì¸ì§€ í™•ì¸"""
        error_str = str(error).lower()
        quota_keywords = [
            "quota exceeded",
            "daily limit exceeded",
            "daily quota exceeded",
            "exceeded your current quota",
            "daily usage limit exceeded",
            "requests per day exceeded",
            "current quota"
        ]
        return any(keyword in error_str for keyword in quota_keywords)

    def _extract_json_from_response(self, response: str) -> Optional[List[Dict]]:
        """LLM ì‘ë‹µì—ì„œ JSON ë°°ì—´ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # ì‘ë‹µì´ ì´ë¯¸ JSON ë°°ì—´ì¸ ê²½ìš°
            if response.strip().startswith('['):
                return json.loads(response.strip())

            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ
            import re
            json_match = re.search(r'```(?:json)?\s*(\[.*?\])\s*```', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))

            # JSON ë°°ì—´ íŒ¨í„´ ì§ì ‘ ì¶”ì¶œ
            json_match = re.search(r'(\[[\s\S]*\])', response)
            if json_match:
                return json.loads(json_match.group(1))

            return None

        except json.JSONDecodeError as e:
            self.logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None


def save_results_with_metadata(output_path: str, data: List[Dict], metadata: Dict[str, Any]) -> None:
    """ê²°ê³¼ ë°ì´í„°ë¥¼ ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    result = {
        "metadata": {
            "created_at": datetime.now().isoformat(),
            "total_input_count": metadata.get("total_input_count", 0),
            "total_output_count": len(data),
            "overall_compression_ratio": metadata.get("overall_compression_ratio", 0),
            "successful_batches": metadata.get("successful_batches", 0),
            "total_batches": metadata.get("total_batches", 0),
            "average_processing_time": metadata.get("average_processing_time", 0),
            "processing_summary": metadata.get("processing_summary", {})
        },
        "data": data
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # ë¡œê±° ì´ˆê¸°í™”
        logger = Logger()
        logger.info("=== EUAA ë¬¸ì¥ êµ¬ì„± ì‹œìŠ¤í…œ ì‹œì‘ ===")

        # íŒŒì¼ ìŠ¤ìºë„ˆ ì´ˆê¸°í™”
        file_processor = FileProcessor()
        sentence_composer = SentenceComposer(logger)

        # ì…ë ¥ íŒŒì¼ë“¤ ìŠ¤ìº”
        logger.info("ì…ë ¥ íŒŒì¼ ìŠ¤ìº” ì¤‘...")
        input_files = file_processor.scan_input_files()

        if not input_files:
            logger.error("ì²˜ë¦¬í•  ì…ë ¥ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        logger.info(f"ë°œê²¬ëœ íŒŒì¼: {len(input_files)}ê°œ")
        for file_info in input_files:
            logger.info(f"  - {file_info['filename']}: {file_info['object_count']:,}ê°œ ê°ì²´")

        # ê° íŒŒì¼ë³„ë¡œ ì²˜ë¦¬
        for file_info in input_files:
            logger.info(f"\nì²˜ë¦¬ ì‹œì‘: {file_info['filename']}")

            # ì…ë ¥ ë°ì´í„° ë¡œë“œ
            with open(file_info['file_path'], 'r', encoding='utf-8') as f:
                input_data = json.load(f)

            part_info = f"{file_info['part_number']}_{file_info['language']}"

            # ë°°ì¹˜ í¬ê¸° ì„¤ì • (H1/H2/H3ê°€ ë§ì•„ì„œ ì„¹ì…˜ë‹¹ Pê°€ 200ê°œ ë„˜ì§€ ì•ŠìŒ)
            batch_size = 200
            logger.info(f"ì–¸ì–´: {file_info['language']}, ë°°ì¹˜ í¬ê¸°: {batch_size}")

            # ì„¹ì…˜ ê¸°ë°˜ ì²˜ë¦¬ (H1/H2/H3ëŠ” ì§ì ‘ ì¶”ê°€, Pë§Œ LLM ì²˜ë¦¬)
            final_results, metadata, all_headers = sentence_composer.process_file(
                input_data,
                part_info,
                batch_size=batch_size
            )

            # ë©”íƒ€ë°ì´í„° í™•ì¥ (save_results_with_metadataì—ì„œ í•„ìš”í•œ í•„ë“œ ì¶”ê°€)
            extended_metadata = {
                "total_input_count": metadata["total_input_count"],
                "overall_compression_ratio": metadata["overall_compression_ratio"],
                "successful_batches": 0,  # ì„¹ì…˜ ê¸°ë°˜ ì²˜ë¦¬ì—ì„œëŠ” ì˜ë¯¸ ì—†ìŒ
                "total_batches": 0,  # ì„¹ì…˜ ê¸°ë°˜ ì²˜ë¦¬ì—ì„œëŠ” ì˜ë¯¸ ì—†ìŒ
                "average_processing_time": 0,  # ì„¹ì…˜ ê¸°ë°˜ ì²˜ë¦¬ì—ì„œëŠ” ì˜ë¯¸ ì—†ìŒ
                "processing_summary": {
                    "total_sections": metadata["total_sections"],
                    "h1_count": metadata["h1_count"],
                    "h2_count": metadata["h2_count"],
                    "h3_count": metadata["h3_count"],
                    "input_p_count": metadata["input_p_count"],
                    "output_p_count": metadata["output_p_count"]
                }
            }

            # ê²°ê³¼ ì €ì¥ (ë©”íƒ€ë°ì´í„° í¬í•¨)
            output_path = file_info['output_path']
            save_results_with_metadata(output_path, final_results, extended_metadata)

            logger.success(f"ì €ì¥ ì™„ë£Œ: {output_path}")
            logger.info(f"   ì›ë³¸: {len(input_data):,}ê°œ â†’ ê²°ê³¼: {len(final_results):,}ê°œ")
            logger.info(f"   P ì••ì¶•ë¥ : {metadata['overall_compression_ratio']:.1f}ë°°")
            logger.info(f"   H1 ë³´ì¡´: {metadata['h1_count']}ê°œ (100%)")
            logger.info(f"   H2 ë³´ì¡´: {metadata['h2_count']}ê°œ (100%)")
            logger.info(f"   H3 ë³´ì¡´: {metadata['h3_count']}ê°œ (100%)")

            # H1/H2/H3 anchor íŒŒì¼ ì €ì¥
            anchor_filename = file_info['filename'].replace('llm_input_', 'anchors_')
            anchor_path = str(file_processor.sentences_dir / anchor_filename)
            with open(anchor_path, 'w', encoding='utf-8') as f:
                json.dump(all_headers, f, ensure_ascii=False, indent=2)

            logger.success(f"Anchor íŒŒì¼ ì €ì¥: {anchor_filename}")
            logger.info(f"   ì´ {len(all_headers)}ê°œ í—¤ë” (H1: {metadata['h1_count']}, H2: {metadata['h2_count']}, H3: {metadata['h3_count']})")

        logger.success("ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")

    except Exception as e:
        if 'logger' in locals():
            logger.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        else:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()